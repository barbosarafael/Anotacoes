{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arvore_de_decisao",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSMkQ2SOcLAo",
        "colab_type": "text"
      },
      "source": [
        "# Anotações e um projeto com Árvore de Decisão em Python\n",
        "\n",
        "\n",
        "- Tentarei focar na prática e dispensar um pouco a teoria com fórmulas. Entretanto, se for algo que vejo que é útil para o entendimento, com certeza entretará;\n",
        "\n",
        "- LinkedIn: https://www.linkedin.com/in/rafael-barbosa0/\n",
        "\n",
        "- Github: https://github.com/barbosarafael\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img alt=\"decision_tree\" src=\"https://i1.wp.com/www.vooo.pro/insights/wp-content/uploads/2016/12/RDS-Vooo_insights-Tutorial_arvore_de_decisao_02.jpg?resize=768%2C446&ssl=1\" alt=\"drawing\" width=\"600\" height=\"400\"/>\n",
        "  <br>\n",
        "    <em> Retirado de: https://www.vooo.pro/insights/um-tutorial-completo-sobre-a-modelagem-baseada-em-tree-arvore-do-zero-em-r-python/ </em>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwmssNpfrnWd",
        "colab_type": "text"
      },
      "source": [
        "# 1. Introdução\n",
        "\n",
        "---\n",
        "\n",
        "- É um modelo de aprendizado de máquina **supervisionado** que serve tanto para classificação quanto para regressão, isto é, precisam de variáveis explicativas para prever uma variável target;\n",
        "\n",
        "- Também serve como visualização para tomada de decisões, já que ela parece um fluxograma;\n",
        "\n",
        "- As variáveis explicativas podem ser de qualquer tipo, seja ela quantitativa ou qualitativa;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCQOsUhNrtWy",
        "colab_type": "text"
      },
      "source": [
        "# 2. Terminologia\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. **Nó raiz**: O ponto de partida de toda árvore (mais pra frente vemos como é feita a escolha desta variável), ele possui maior nível hierárquico e faz a ligação inicial para outros elementos;\n",
        "\n",
        "2. **Divisão**: É o processo de dividir um nó em dois ou mais sub-nós;\n",
        "\n",
        "3. **Nó de decisão**: Quando dividimos os nós (ou sub-nós) em sub-nós adicionais, através da condição **se, então**;\n",
        "\n",
        "4. **Folha ou nó de término**: São os nós em que não ocorrem mais divisões, estes podem ser os últimos da árvore e contém um valor contínuo ou categórico (a categoria final escolhida);\n",
        "\n",
        "5. **Poda**: Processo de remover alguns dos elementos da árvore final, podendo ser nós, sub-nós, etc.\n",
        "\n",
        "6. **Nó pai e filho**: Nó divido entre sub-nós. O nó é chamado de pai e os sub-nós chamados de filhos.\n",
        "\n",
        "> Resumo: A árvore armazena regras em seus nós (**se, então**) e, baseado nas regras, temos uma decisão que será armazenada na **folha ou nó de término**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XZv6uRLygom",
        "colab_type": "text"
      },
      "source": [
        "# 3. Construção da árvore, Divisão dos nós e Impureza\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img alt=\"decision_tree\" src=\"https://www.model-railroad-infoguy.com/images/suncoasttree.jpg\" alt=\"drawing\" width=\"300\" height=\"300\"/>\n",
        "  <br>\n",
        "    <em> Retirado de: https://www.model-railroad-infoguy.com/trees.html </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "- Ela particiona o espaço gerado pelas variáveis explicativas ($ X_i $) em algumas regiões para que estas partições sejam as mais puras possíveis;\n",
        "\n",
        "- A árvore divide os nós em todas as variáveis que estão disponíveis e seleciona uma partição que resulta em sub-nós mais homogêneos (relacionados a pureza);\n",
        "\n",
        "- **Pureza**: Se todos os elementos pertencem a uma única classe/categoria então ele é chamado de puro;\n",
        "\n",
        "- Devido a profundidade e a divisão da árvore, ela pode aprender demais e acabar sofrendo o **overfitting** (quando o modelo aprende o padrão total/perfeito dos dados nos dados de treino e quando pedimos para testar em dados diferentes acaba por dar erros absurdos);\n",
        "\n",
        "- Esse problema pode ser contornado a partir da **poda** da árvore e sobre o controle dos parâmetros;\n",
        "\n",
        "- Podemos controlar a pureza por meio de 4 métodos:\n",
        "\n",
        "  1. Índice Gini;\n",
        "  2. Entropia e Ganho de informação;\n",
        "  3. Qui quadrado;\n",
        "  4. Redução na variância.\n",
        "\n",
        "- No presente notebook, explico somente as primeiras duas, sendo elas as mais conhecidas e implementadas nos softwares atuais.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUnXD11jzyfT",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Índice de Gini\n",
        "\n",
        "- Mede o grau de homogeneidade dos dados. Por isso, pode ser usado para calcular a pureza dos nós;\n",
        "\n",
        "- Em um determinado nó (podendo ser raiz ou divisão qualquer), o índice é dado por:\n",
        "\n",
        "$$ IG = 1 - \\sum_{i = 1}^{c} p_{i}^{2} $$\n",
        "\n",
        "onde $ p_{i} $ é a frequência de cada categoria em cada nó e c o número de categorias.\n",
        "\n",
        "- Quando o IG é 0, indica que todos as observações pertencem a somente uma categoria ou existe somente uma;\n",
        "\n",
        "- Quando se aproxima de 1, indica que as observações são distribuídas aleatoriamente nas categorias;\n",
        "\n",
        "- Quando o valor é de 0,5 indica que as observações são igualmente distribuídas nas categorias;\n",
        "\n",
        "- Executa divisões binárias, isto é, vai para esquerda ou direita;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuuevZlixc4s",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Entropia e Ganho de Informação\n",
        "\n",
        "- A Entropia caracteriza a impureza dos dados, sendo também uma medida de verificação de homogeneidade dos dados em relação a sua classificação;\n",
        "\n",
        "- Novamente o conceito de **pureza**: Se todos os elementos pertencem a uma única classe/categoria então ele é chamado de puro;\n",
        "\n",
        "- Quanto mais puro é um nó, menor a quantidade de informação que ele precisa para ser descrito e vice-versa;\n",
        "\n",
        "- Por meio da teoria da informação, podemos medir esse grau de desorganização/impureza que é a entropia. A fórmula é dada por \n",
        "\n",
        "$$ Entropia(S) = \\sum_{i = 1}^{c} - p_i \\log_{2}(p_i) $$\n",
        "\n",
        "- Podemos notar a partir do gráfico que:\n",
        "\n",
        "  - Se a entropia for completamente homogênea, a entropia será 0 (extremidades);\n",
        "\n",
        "  - Se as categorias possuem a mesma probabilidade, isto é 50/50, então a entropia será 1 (máxima);\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img alt=\"decision_tree\" src=\"https://sites.google.com/site/mansoldo1986/_/rsrc/1472876171683/home/leavesphp/classificacao-por-arvores-de-decisao/criteriodeselecaodeatributos/entropia%20graph1.jpg\n",
        "\" alt=\"drawing\" width=\"400\" height=\"300\">\n",
        "  <br>\n",
        "    <em> Retirado de: https://sites.google.com/site/mansoldo1986/home/leavesphp/classificacao-por-arvores-de-decisao </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "- O ganho de informação pode ser baseado na entropia. Nesse caso, **há um ganho de informação quando em alguma divisão dos nós nos retorna uma redução na entropia**;\n",
        "\n",
        "- Se as probabilidades para o evento tenderem a somente uma categoria da variável, i.e. 99% de chance para categoria1 e 1% para categoria2, então teremos quase **nenhum ganho de informação** pois já sabemos que é bastante provavel uma categoria;\n",
        "\n",
        "- Se as probabilidade para as mesmas categorias serem um pouco mais iguais, i.e. 60/40, 50/50, 55/45, etc, temos uma \"surpresa\" em um possível resultado, pois não sabemos realmente qual categoria será a escolhida. Logo, teremos um **ganho de informação**. \n",
        "\n",
        "- Novamente, o ganho de informação é baseado na redução da entropia após a contrução de um nó. O ideal é que os nós sejam os mais homogêneos possíveis para que eles nos retornem um maior ganho de informação, logo uma menor entropia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQsKU5r1GPU4",
        "colab_type": "text"
      },
      "source": [
        "# 4. *Overfitting*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img alt=\"decision_tree\" src=\"https://miro.medium.com/max/1400/1*cdvfzvpkJkUudDEryFtCnA.png\" width=\"700\" height=\"300\">\n",
        "  <br>\n",
        "    <em> Retirado de: https://medium.com/@srjoglekar246/overfitting-and-human-behavior-5186df1e7d19 </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "- Como podemos observar na Figura acima, o overfitting é o problema que temos quando o modelo aprende demais o padrão nos dados de treino, tendo erro quase nulo, e em dados novos o erro acaba por aumentar absurdamente;\n",
        "\n",
        "- Em modelos de árvore, se não controlarmos seus parâmetros, ela simplesmente irá criar uma folha (atributo final) para cada observação, fazendo com que árvore seja imensa, englobando todas as possibilidades, como no exemplo abaixo;\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img alt=\"decision_tree\" src=\"https://i.stack.imgur.com/otsP3.png\" width=\"400\" height=\"300\">\n",
        "  <br>\n",
        "    <em>Retirado de: https://stats.stackexchange.com/questions/230581/decision-tree-too-large-to-interpret</em>\n",
        "</p>\n",
        "\n",
        "- Podemos contornar esse problema de *overfitting* na árvore de duas maneiras:\n",
        "\n",
        "  - Controlando os parâmetros da árvore de decisão (antes ou depois da aplicação do modelo);\n",
        "  \n",
        "  - Podando a árvore (após a aplicação do modelo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dMiAxSzOQhK",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. Parâmetros da árvore de decisão\n",
        "\n",
        "- Estes parâmetros nos dizem ao tamanho da nossa árvore;\n",
        "\n",
        "- Utilizaremos os conceitos do `sklearn` em Python;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gYaTaBQ-12y",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.1. Max_depth\n",
        "\n",
        "- Maior profundidade permitida da árvore de decisão verticalmente;\n",
        "\n",
        "- Conforme o número deste parâmetro aumenta, a complexidade da árvore também, junto com a possibilidade de overfitting. Já que a árvore será \"grande\" e irá aprender todos os padrões dos dados;\n",
        "\n",
        "- O valor **default** deste parâmetro no `sklearn` é de None (Nenhum) mas é controlado por outro parâmetro chamado de `min_samples_split`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mK7iz7LC03E",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.2. Min_samples_split\n",
        "\n",
        "- Número mínimo de amostras/observações para dividir um nó;\n",
        "\n",
        "- O valor **default** deste parâmetro no `sklearn` é de 2 observações, isto é, \n",
        "a partir de 2 observações já pode dividir em um nó;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ctmwl38GPOs",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.3. Min_samples_leaf\n",
        "\n",
        "- Número mínimo de amostras/observações necessárias para o último nó (folha);\n",
        "\n",
        "- O valor **default** deste parâmetro no `sklearn` é de 1 observação;\n",
        "\n",
        "- Deve-se deixar pelo menos 1 observação em cada um dos ramos, seja ele esquerdo ou direito, para ser um ponto de divisão (como vimos no parâmetro acima);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkchKhYJCLMj",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.4. max_features\n",
        "\n",
        "- Número de variáveis/features utilizadas para tomar a decisão (esquerda ou direita);\n",
        "\n",
        "- Dado que é **difícil ou custoso** olhar para todos os seus dados e variáveis em todas as divisões, se você tiver 35 variáveis e setar esse parâmetro como 10, ele selecionará 10 variáveis aleatoriamente para escolher a próxima divisão, e assim por diante;\n",
        "\n",
        "- O valor **default** deste parâmetro no `sklearn` é \"Nenhum\". Logo, ele irá todos seus dados, mas existem opções, elas são:\n",
        "\n",
        "  - \"Auto\", então max_features = $ \\sqrt{n\\_features} $.\n",
        "\n",
        "  - \"Sqrt\", então max_features = $ \\sqrt{n\\_features} $.\n",
        "\n",
        "  - \"log2\", então max_features = $ \\log_{2}(n\\_features) $.\n",
        "\n",
        "  - \"None\", então max_features = n_features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yySR8TFGHTCg",
        "colab_type": "text"
      },
      "source": [
        "## 4.2. Poda da Árvore (Pruning)\n",
        "\n",
        "- Pré-podagem: Parar o crescimento da árvore mais cedo;\n",
        "\n",
        "- Pós-podagem: Acontece quando a árvore já está completa;\n",
        "\n",
        "- Pode ser feito da seguinte maneira:\n",
        "\n",
        "  1. Percorre a ávore em profundidade;\n",
        "\n",
        "  2. Para cada nó de decisão, calcula: \n",
        "\n",
        "    - Erro do nó;\n",
        "\n",
        "    - Soma dos erros dos nós descendentes.\n",
        "\n",
        "  3. Se o erro do nó é menor ou igual à soma dos erros dos nós descendentes então o nó é transformado em folha. \n",
        "\n",
        "- Valorizando a pureza;\n",
        "\n",
        "- Na pós-podagem, a sub-árvore com melhor desempenho é escolhida;\n",
        "\n",
        "- Segundo este [link](https://www.vooo.pro/insights/um-tutorial-completo-sobre-a-modelagem-baseada-em-tree-arvore-do-zero-em-r-python/), no `sklearn` não existe a implementação da podageme, entretanto no R tem a função `prune` do pacote `rpart`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtkhGyBULQXp",
        "colab_type": "text"
      },
      "source": [
        "# 5. Quando utilizar modelos baseados em árvore ?\n",
        "\n",
        "---\n",
        "\n",
        "- Quando a relação das variáveis explicativas com a target não é linear;\n",
        "\n",
        "- Uma segunda opinião, segundo o [Mário Filho](https://www.mariofilho.com/). Na dúvida entre modelos, teste todos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fTNXeLzNT7R",
        "colab_type": "text"
      },
      "source": [
        "# 6. Um exemplo de aplicação em Python\n",
        "\n",
        "---\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du6ZLsomhzeG",
        "colab_type": "text"
      },
      "source": [
        "# Referências\n",
        "\n",
        "---\n",
        "\n",
        "1. Elements of Statistical Learning\n",
        "2. Introdução à Ciência de Dados Fundamentos e Aplicações\n",
        "3. https://medium.com/machine-learning-beyond-deep-learning/%C3%A1rvores-de-decis%C3%A3o-3f52f6420b69\n",
        "4. https://www.wrprates.com/o-que-e-arvore-de-decisao-decision-tree-linguagem-r/\n",
        "5. https://www.maxwell.vrac.puc-rio.br/7587/7587_4.PDF\n",
        "6. https://edisciplinas.usp.br/pluginfile.php/4469825/mod_resource/content/1/ArvoresDecisao_normalsize.pdf\n",
        "7. https://www.cin.ufpe.br/~pacm/SI/ArvoreDecisaoIndutiva.pdf\n",
        "8. https://www.vooo.pro/insights/um-tutorial-completo-sobre-a-modelagem-baseada-em-tree-arvore-do-zero-em-r-python/\n",
        "9. https://minerandodados.com.br/arvores-de-decisao-conceitos-e-aplicacoes/\n",
        "10. https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree\n",
        "11. Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
        "12. https://www.alura.com.br/conteudo/machine-learning-otimizacao-de-modelos-atraves-de-hiperparametros"
      ]
    }
  ]
}